\section{Semantic Entropy Formalism}
\label{sec:semantic-entropy}

We formalize semantic entropy $E(\sigma)$ as a composite measure combining internal inconsistency, underdetermination, and mutual-information entanglement between modalities.  Practically this is computed as:
\[
E(\sigma) = \lambda_{\mathrm{inc}}\cdot H_{\mathrm{inc}}(\sigma) + \lambda_{\mathrm{mi}}\cdot \sum_{i<j}\mathrm{MI}(M(k_i),M(k_j)) + \lambda_{\mathrm{prov}}\cdot \mathrm{Uncert}(S)
\]
where each term is normalized; $\lambda$ coefficients calibrate domain weighting.

\subsection{Entropy inequalities and Landauer bound}
We prove a semantic Landauer-like bound: any irreversible transformation that lowers representational redundancy incurs a minimal entropy production proportional to information erased.  This gives lower bounds on $\epsilon_r$ per rule that performs lossy summarization.

\subsection{Measurement and estimation}
We outline practical estimators: cross-modal reconstruction error, classifier disagreement, and mutual information proxies (MINE, kNN estimators).  These yield computable $E(\sigma)$ for experiments in \Cref{sec:experiments}.
